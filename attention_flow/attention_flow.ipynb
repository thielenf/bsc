{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "import numpy as np\n",
    "# import scipy\n",
    "import glob\n",
    "import json\n",
    "from attention_graph_util import compute_flows, create_attention_graph, get_adjmat\n",
    "\n",
    "from helpers import get_edit_positions, get_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# %%\n",
    "# https://gist.github.com/jlherren/d97839b1276b9bd7faa930f74711a4b6\n",
    "\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\") or \"bert-base-cased\"\n",
    "ATTENTION_TYPE = (\n",
    "    os.getenv(\"ATTENTION_TYPE\") or \"attentions\"\n",
    ")  # \"/media/data/models/bert-base-cased\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /media/data/thielen/ba/negation_datasets/bert-base-cased/attentions/LAMA_negated/Squad/train/32\n",
      "attentions.shape torch.Size([12, 12, 32, 32])\n",
      "Loaded 1 dataframes\n"
     ]
    }
   ],
   "source": [
    "def get_target_attentions(item):\n",
    "    try:\n",
    "        unmasked: torch.Tensor = item.attentions_sentence[\n",
    "            :, :, :, item.tokenized_edited_positions_in_unmasked\n",
    "        ]\n",
    "        negated: torch.Tensor = item.attentions_negated[\n",
    "            :, :, :, item.tokenized_edited_positions_in_negated\n",
    "        ]\n",
    "        negated_with_original: torch.Tensor = item.attentions_negated[\n",
    "            :, :, :, item.tokenized_edited_positions_in_negated_with_original\n",
    "        ]\n",
    "    except IndexError:\n",
    "        print(item)\n",
    "        print(item.tokenized_edited_positions_in_unmasked)\n",
    "        print(item.tokenized_edited_positions_in_negated)\n",
    "        print(item.attentions_sentence.shape)\n",
    "        print(item.attentions_negated.shape)\n",
    "        raise\n",
    "    return unmasked, negated, negated_with_original\n",
    "\n",
    "\n",
    "# %%\n",
    "def load(dataset_path: str) -> pd.DataFrame:\n",
    "    # if \"LAMA_negated\" in dataset_path:\n",
    "    #     for df in get_dataset():\n",
    "    #         dataset = df.iloc[0,][\"dataset\"]\n",
    "    #         for attention_type in glob.glob(\n",
    "    #             f\"/media/data/thielen/ba/negation_datasets/{os.path.basename(MODEL_NAME)}/{ATTENTION_TYPE}/LAMA_negated/{dataset}/*/*\"\n",
    "    #         ):\n",
    "    #             if not os.path.exists(f\"{attention_type}/variant_00/attentions.pt\"):\n",
    "    #                 # print(f\"Skipping {attention_type}\")\n",
    "    #                 continue\n",
    "\n",
    "    #             # print(f\"Loading {attention_type}\")\n",
    "    #             df[\"attentions_sentence\"] = [\n",
    "    #                 row\n",
    "    #                 for row in torch.load(\n",
    "    #                     os.path.join(attention_type, \"variant_00\", \"attentions.pt\")\n",
    "    #                 )\n",
    "    #             ]\n",
    "    #             df[\"pooler_outputs_sentence\"] = [\n",
    "    #                 row for row in torch.load(f\"{attention_type}/variant_00/pooler_outputs.pt\")\n",
    "    #             ]\n",
    "    #             df[\"attentions_negated\"] = [\n",
    "    #                 row\n",
    "    #                 for row in torch.load(\n",
    "    #                     os.path.join(attention_type, \"variant_01\", \"attentions.pt\")\n",
    "    #                 )\n",
    "    #             ]\n",
    "    #             df[\"pooler_outputs_negated\"] = [\n",
    "    #                 row for row in torch.load(f\"{attention_type}/variant_01/pooler_outputs.pt\")\n",
    "    #             ]\n",
    "    #             df[\"dataset_path\"] = attention_type\n",
    "\n",
    "    #             print(\"attentions.shape\", df[\"attentions_sentence\"][0].shape)\n",
    "\n",
    "    #             yield df\n",
    "    #             # return\n",
    "    # else:\n",
    "        # return\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for attention_type in glob.glob(f\"{dataset_path}/*/*/*\"):\n",
    "        if \"Squad\" not in attention_type:\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(f\"{attention_type}/variant_00/attentions.pt\"):\n",
    "            print(f\"Skipping {attention_type}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Loading {attention_type}\")\n",
    "\n",
    "        df[\"unmasked_sentences\"] = pd.read_csv(\n",
    "            f\"{attention_type}/variant_00/samples.csv\"\n",
    "        )[\"Sentence\"]\n",
    "        df[\"unmasked_negated\"] = pd.read_csv(\n",
    "            f\"{attention_type}/variant_01/samples.csv\"\n",
    "        )[\"Sentence\"]\n",
    "\n",
    "        df[\"attentions_sentence\"] = [\n",
    "            row for row in torch.load(f\"{attention_type}/variant_00/attentions.pt\")\n",
    "        ]\n",
    "        df[\"pooler_outputs_sentence\"] = [\n",
    "            row for row in torch.load(f\"{attention_type}/variant_00/pooler_outputs.pt\")\n",
    "        ]\n",
    "        df[\"attentions_negated\"] = [\n",
    "            row for row in torch.load(f\"{attention_type}/variant_01/attentions.pt\")\n",
    "        ]\n",
    "        df[\"pooler_outputs_negated\"] = [\n",
    "            row for row in torch.load(f\"{attention_type}/variant_01/pooler_outputs.pt\")\n",
    "        ]\n",
    "        df[\"dataset_path\"] = attention_type\n",
    "\n",
    "        print(\"attentions.shape\", df[\"attentions_sentence\"][0].shape)\n",
    "\n",
    "        yield df\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for dataset_path in glob.glob(\n",
    "    f\"/media/data/thielen/ba/negation_datasets/{os.path.basename(MODEL_NAME)}/{ATTENTION_TYPE}/*\"\n",
    "):\n",
    "    # print(f\"Loading {dataset_path}\")\n",
    "    dfs.extend(load(dataset_path))  # extend implicitly iterates over the generator\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    if \"masked_sentences\" in dfs[i].columns:\n",
    "        dfs[i][\"unmasked_sentences\"] = dfs[i].apply(\n",
    "            lambda s: s[\"masked_sentences\"].replace(\"[MASK]\", str(s[\"obj_label\"])),\n",
    "            axis=1,\n",
    "        )\n",
    "        dfs[i][\"unmasked_negated\"] = dfs[i].apply(\n",
    "            lambda s: s[\"negated\"].replace(\"[MASK]\", str(s[\"obj_label\"])), axis=1\n",
    "        )\n",
    "\n",
    "print(f\"Loaded {len(dfs)} dataframes\")\n",
    "\n",
    "\n",
    "def find_sub_list(sl, l):\n",
    "    results = []\n",
    "    sll = len(sl)\n",
    "    for ind in (i for i, e in enumerate(l) if e == sl[0]):\n",
    "        if l[ind : ind + sll] == sl:\n",
    "            # results.append((ind,ind+sll-1))\n",
    "            results.append(list(range(ind, ind + sll)))\n",
    "\n",
    "    return [x for y in results for x in y]\n",
    "\n",
    "\n",
    "def maybe_add_original_positions(item):\n",
    "    if (\n",
    "        not len(item.tokenized_edited_tokens_in_negated)\n",
    "        or item.tokenized_edited_tokens_in_negated[0] not in [\"not\", \"cannot\"]\n",
    "        or item.tokenized_edited_positions_in_negated\n",
    "        == item.tokenized_edited_positions_in_unmasked\n",
    "    ):\n",
    "        return item.tokenized_edited_positions_in_negated\n",
    "    else:\n",
    "        ret = (\n",
    "            item.tokenized_edited_positions_in_negated\n",
    "            + item.tokenized_edited_positions_in_unmasked\n",
    "        )\n",
    "        return list(sorted(set(ret)))\n",
    "\n",
    "\n",
    "def maybe_add_original_tokens(item):\n",
    "    if (\n",
    "        not len(item.tokenized_edited_tokens_in_negated)\n",
    "        or item.tokenized_edited_tokens_in_negated[0] not in [\"not\", \"cannot\"]\n",
    "        or item.tokenized_edited_positions_in_negated\n",
    "        == item.tokenized_edited_positions_in_unmasked\n",
    "    ):\n",
    "        return item.tokenized_edited_tokens_in_negated\n",
    "    else:\n",
    "        ret = (\n",
    "            item.tokenized_edited_tokens_in_negated\n",
    "            + item.tokenized_edited_tokens_in_unmasked\n",
    "        )\n",
    "        return list(sorted(set(ret), key=ret.index))\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def tokenize(input):\n",
    "    return TOKENIZER.tokenize(\n",
    "        input,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    dfs[i][\"tokenized_unmasked_sentences\"] = dfs[i].apply(\n",
    "        lambda s: tokenize(s[\"unmasked_sentences\"]), axis=1\n",
    "    )\n",
    "    dfs[i][\"tokenized_unmasked_negated\"] = dfs[i].apply(\n",
    "        lambda s: tokenize(s[\"unmasked_negated\"]), axis=1\n",
    "    )\n",
    "\n",
    "    dfs[i][\"tokenized_edited_positions_in_negated\"] = dfs[i].apply(\n",
    "        lambda s: get_edit_positions(\n",
    "            [\n",
    "                tok\n",
    "                for tok in s[\"tokenized_unmasked_sentences\"]\n",
    "                if tok not in TOKENIZER.all_special_tokens\n",
    "            ],\n",
    "            [\n",
    "                tok\n",
    "                for tok in s[\"tokenized_unmasked_negated\"]\n",
    "                if tok not in TOKENIZER.all_special_tokens\n",
    "            ],\n",
    "        )[: s[\"attentions_sentence\"].shape[-1]],\n",
    "        axis=1,\n",
    "    )\n",
    "    dfs[i][\"tokenized_edited_tokens_in_negated\"] = dfs[i].apply(\n",
    "        lambda s: [\n",
    "            list(filter(lambda t: t not in TOKENIZER.all_special_tokens, s[\"tokenized_unmasked_negated\"]))[idx]\n",
    "            for idx in s[\"tokenized_edited_positions_in_negated\"]\n",
    "            if idx < len(list(filter(lambda t: t not in TOKENIZER.all_special_tokens, s[\"tokenized_unmasked_negated\"])))\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    dfs[i][\"tokenized_edited_positions_in_unmasked\"] = dfs[i].apply(\n",
    "        lambda s: get_edit_positions(\n",
    "            [\n",
    "                tok\n",
    "                for tok in s[\"tokenized_unmasked_negated\"]\n",
    "                if tok not in TOKENIZER.all_special_tokens\n",
    "            ],\n",
    "            [\n",
    "                tok\n",
    "                for tok in s[\"tokenized_unmasked_sentences\"]\n",
    "                if tok not in TOKENIZER.all_special_tokens\n",
    "            ],\n",
    "        )[: s[\"attentions_sentence\"].shape[-1]],\n",
    "        axis=1,\n",
    "    )\n",
    "    dfs[i][\"tokenized_edited_tokens_in_unmasked\"] = dfs[i].apply(\n",
    "        lambda s: [\n",
    "            list(filter(lambda t: t not in TOKENIZER.all_special_tokens, s[\"tokenized_unmasked_sentences\"]))[idx]\n",
    "            for idx in s[\"tokenized_edited_positions_in_unmasked\"]\n",
    "            if idx < len(list(filter(lambda t: t not in TOKENIZER.all_special_tokens, s[\"tokenized_unmasked_sentences\"])))\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    dfs[i][\"tokenized_edited_tokens_in_negated_with_original\"] = dfs[i].apply(\n",
    "        maybe_add_original_tokens, axis=1\n",
    "    )\n",
    "    dfs[i][\"tokenized_edited_positions_in_negated_with_original\"] = dfs[i].apply(\n",
    "        maybe_add_original_positions, axis=1\n",
    "    )\n",
    "\n",
    "    dfs[i] = dfs[i][\n",
    "        dfs[i][\"tokenized_edited_tokens_in_negated\"].apply(len) > 0\n",
    "    ].reset_index(drop=True)\n",
    "    dfs[i] = dfs[i][\n",
    "        dfs[i][\"tokenized_edited_tokens_in_unmasked\"].apply(len) > 0\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    dfs[i][\"model_name\"] = MODEL_NAME\n",
    "\n",
    "# %%\n",
    "for i in range(len(dfs)):\n",
    "    index = dfs[i][\n",
    "        dfs[i].unmasked_sentences\n",
    "        == \"Three humans are in an airport. One is on the ground, another is next to the window, and the last one is vertical on his two feet with a bag next to him.\"\n",
    "    ]\n",
    "    if len(index):\n",
    "        dfs[i] = dfs[i].drop(index.index)\n",
    "\n",
    "    index = dfs[i][\n",
    "        dfs[i].tokenized_edited_positions_in_unmasked.map(max)\n",
    "        > dfs[i].attentions_sentence.map(lambda x: x.shape[-1])\n",
    "    ]\n",
    "    if len(index):\n",
    "        dfs[i] = dfs[i].drop(index.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flow(item, _type):\n",
    "    device = \"cpu\" #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # \"attentions_sentence\", \"attentions_negated\"\n",
    "    attentions_mat_regular = (item[_type]).clone().detach().to(device)\n",
    "    res_att_mat = attentions_mat_regular.sum(axis=1) / attentions_mat_regular.shape[1] # average over heads\n",
    "    res_att_mat = res_att_mat + torch.eye(res_att_mat.shape[1])[None,...].to(device) # add identity matrix to avoid division by zero\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None] # normalize: sum of each row = 1\n",
    "    tokens = item[\"tokenized_unmasked_sentences\"] if _type == \"attentions_sentence\" else item[\"tokenized_unmasked_negated\"]\n",
    "    # print(res_att_mat.shape)\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=tokens)\n",
    "    res_G = create_attention_graph(res_adj_mat)\n",
    "    output_nodes = []\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if 'L24' in key:\n",
    "            output_nodes.append(key)\n",
    "        if res_labels_to_index[key] < attentions_mat_regular.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    flow_values = compute_flows(res_G, res_labels_to_index, input_nodes, length=attentions_mat_regular.shape[-1])\n",
    "    # print(flow_values.shape)\n",
    "    return flow_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmasked_sentences                                     To emphasize the 50th anniversary of the Super...\n",
      "unmasked_negated                                       To emphasize the 50th anniversary of the Super...\n",
      "attentions_sentence                                    [[[tensor([0.5185, 0.0464, 0.0057, 0.0171, 0.0...\n",
      "pooler_outputs_sentence                                [[tensor(-0.6894), tensor(0.4977), tensor(0.99...\n",
      "attentions_negated                                     [[[tensor([0.5304, 0.0475, 0.0059, 0.0175, 0.0...\n",
      "pooler_outputs_negated                                 [[tensor(-0.6308), tensor(0.4733), tensor(0.99...\n",
      "dataset_path                                           /media/data/thielen/ba/negation_datasets/bert-...\n",
      "tokenized_unmasked_sentences                           [[CLS], To, emphasize, the, 50th, anniversary,...\n",
      "tokenized_unmasked_negated                             [[CLS], To, emphasize, the, 50th, anniversary,...\n",
      "tokenized_edited_positions_in_negated                                                               [13]\n",
      "tokenized_edited_tokens_in_negated                                                                 [was]\n",
      "tokenized_edited_positions_in_unmasked                                                              [13]\n",
      "tokenized_edited_tokens_in_unmasked                                                                [was]\n",
      "tokenized_edited_tokens_in_negated_with_original                                                   [was]\n",
      "tokenized_edited_positions_in_negated_with_original                                                 [13]\n",
      "model_name                                                                               bert-base-cased\n",
      "Name: 0, dtype: object\n",
      "torch.Size([12, 32, 32])\n",
      "(416, 416)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "item = dfs[0].iloc[idx]\n",
    "print(item)\n",
    "\n",
    "# attentions_sentence, attentions_negated\n",
    "flow = compute_flow(item, \"attentions_negated\")\n",
    "print(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m        mats[i] \u001b[38;5;241m=\u001b[39m adjmat[(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39ml:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39ml,i\u001b[38;5;241m*\u001b[39ml:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39ml]\n\u001b[1;32m      8\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m mats\n\u001b[0;32m---> 10\u001b[0m flow_att_mat \u001b[38;5;241m=\u001b[39m convert_adjmat_tomats(\u001b[43mflow\u001b[49m, n_layers\u001b[38;5;241m=\u001b[39mattentions_mat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], l\u001b[38;5;241m=\u001b[39mattentions_mat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     11\u001b[0m flow_att_mat\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flow' is not defined"
     ]
    }
   ],
   "source": [
    "attentions_mat = item[\"attentions_sentence\"]\n",
    "def convert_adjmat_tomats(adjmat, n_layers, l):\n",
    "   mats = np.zeros((n_layers,l,l))\n",
    "   \n",
    "   for i in np.arange(n_layers):\n",
    "       mats[i] = adjmat[(i+1)*l:(i+2)*l,i*l:(i+1)*l]\n",
    "       \n",
    "   return mats\n",
    "\n",
    "flow_att_mat = convert_adjmat_tomats(flow, n_layers=attentions_mat.shape[0], l=attentions_mat.shape[-1])\n",
    "flow_att_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(dfs[0].dataset_path.iloc[0], \"flows\", f\"{idx}\".rjust(3, \"0\") + \".npy\")\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "np.save(file_path, flow_att_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/586 [00:00<?, ?it/s][Parallel(n_jobs=32)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[Parallel(n_jobs=32)]: Done   8 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=32)]: Done  98 tasks      | elapsed: 149.4min\n",
      "[Parallel(n_jobs=32)]: Done 224 tasks      | elapsed: 360.2min\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed: 673.2min\n",
      "[Parallel(n_jobs=32)]: Done 586 out of 586 | elapsed: 970.8min finished\n",
      "  0%|          | 0/586 [16:10:59<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_item(item, _type):\n",
    "    idx = item.name\n",
    "    flow = compute_flow(item, _type)\n",
    "    attentions_mat = item[_type]\n",
    "    flow_att_mat = convert_adjmat_tomats(\n",
    "        flow, n_layers=attentions_mat.shape[0], l=attentions_mat.shape[-1]\n",
    "    )\n",
    "    file_name = f\"{idx:03}_{_type}.npy\"\n",
    "    file_path = os.path.join(item.dataset_path, \"flows\", file_name)\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    np.save(file_path, flow_att_mat)\n",
    "\n",
    "num_cores = 32\n",
    "start_idx = 9\n",
    "tasks = [(item, _type) for _, item in dfs[0].iloc[start_idx:].iterrows() for _type in [\"attentions_sentence\", \"attentions_negated\"]]\n",
    "\n",
    "with tqdm(total=len(tasks)) as pbar:\n",
    "    def update(*a):\n",
    "        pbar.update()\n",
    "\n",
    "    Parallel(n_jobs=num_cores, prefer=\"processes\", verbose=5)(\n",
    "        delayed(process_item)(item, _type)\n",
    "        for item, _type in tasks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 302/302 [00:00<00:00, 1805.14it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_flows(item):\n",
    "    flows = []\n",
    "    for _type in [\"attentions_sentence\", \"attentions_negated\"]:\n",
    "        filename = os.path.join(item.dataset_path, \"flows\", f\"{item.name:03}_{_type}.npy\")\n",
    "        flows.append(np.load(filename))\n",
    "    return flows\n",
    "\n",
    "# Use a for loop to apply load_flows to each row\n",
    "results = []\n",
    "for _, item in tqdm(dfs[0].iterrows(), total=dfs[0].shape[0]):\n",
    "    results.append(load_flows(item))\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "df_flows = pd.DataFrame(results, columns=[\"flow_sentence\", \"flow_negated\"], index=dfs[0].index)\n",
    "\n",
    "# Concatenate the new DataFrame with the original DataFrame\n",
    "dfs[0] = pd.concat([dfs[0], df_flows], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 32, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0].iloc[0].flow_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 302/302 [00:00<00:00, 2534.02it/s]\n",
      "100%|██████████| 7248/7248 [02:02<00:00, 59.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_and_save_heatmap(att_mat_target, non_pad_tokens, dataset_path, name, attention_type, layer):\n",
    "    fig, ax = plt.subplots(figsize=(6,8))\n",
    "    sns.heatmap(att_mat_target, cmap=\"YlOrRd\", xticklabels=non_pad_tokens, yticklabels=non_pad_tokens, ax=ax)\n",
    "    filepath = os.path.join(dataset_path, \"png\", \"flows\", f\"{name:03}_{attention_type}_layer_{layer:02}.png\")\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    \n",
    "    # Rotate the labels\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.savefig(filepath, bbox_inches='tight')\n",
    "    plt.clf()  # Clear the figure\n",
    "    plt.close(fig)  # Close the figure\n",
    "\n",
    "results = []\n",
    "for _, item in tqdm(dfs[0].iterrows(), total=dfs[0].shape[0]):\n",
    "    for attention_type in [\"attentions_sentence\", \"attentions_negated\"]:\n",
    "        flow_att_mat = item.flow_sentence if attention_type == \"attentions_sentence\" else item.flow_negated\n",
    "        src = item.tokenized_unmasked_sentences if attention_type == \"attentions_sentence\" else item.tokenized_unmasked_negated\n",
    "        sentence = item.unmasked_sentences if attention_type == \"attentions_sentence\" else item.unmasked_negated\n",
    "\n",
    "        non_pad_indices = [i for i in range(len(src)) if src[i] not in [TOKENIZER.sep_token, TOKENIZER.pad_token]]\n",
    "        non_pad_tokens = [src[i] for i in non_pad_indices]\n",
    "\n",
    "        for layer in range(flow_att_mat.shape[0]):\n",
    "            att_mat_target = flow_att_mat[layer]\n",
    "            results.append((att_mat_target, non_pad_tokens, item.dataset_path, item.name, attention_type, layer))\n",
    "\n",
    "# Parallelize the creation and saving of heatmaps\n",
    "Parallel(n_jobs=32)(delayed(create_and_save_heatmap)(*result) for result in tqdm(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
